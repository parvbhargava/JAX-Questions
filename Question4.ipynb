{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question4.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNAy7IWbGYMR+AIJHZh7sC1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parvbhargava/SRIP-Parv-Bhargava/blob/main/Question4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import jax.numpy as jnp\n",
        "from jax import random \n",
        "key = random.PRNGKey(43)"
      ],
      "metadata": {
        "id": "6tl7EBnJMqFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In simple linear regression we compute point estimates of our parameters (e.g. using a maximum likelihood approach) and use these estimates to make predictions. Different to this, Bayesian linear regression estimates distributions over the parameters and predictions. This allows us to model the uncertainty in our predictions.\n",
        "\n",
        "To perform Bayesian linear regression we follow three steps:\n",
        "\n",
        "1. We set up a probabilistic model that describes our assumptions how the data and parameters are generated\n",
        "2. We perform inference for the parameters $θ$ \n",
        ", that is, we compute the posterior probability distribution over the parameters\n",
        "3. With this posterior we can perform inference for new, unseen inputs $y_{⋆} $\n",
        ". In this step we don’t compute point estimates of the outputs. Instead, we compute the parameters of the posterior distribution over the outputs."
      ],
      "metadata": {
        "id": "MUqoKXpGXJxd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPNGbslRMOE6"
      },
      "outputs": [],
      "source": [
        "class BayesianLinearRegression:\n",
        "    \"\"\" Bayesian linear regression\n",
        "    \n",
        "    Args:\n",
        "        prior_mean: Mean values of the prior distribution (m_0)\n",
        "        prior_cov: Covariance matrix of the prior distribution (S_0)\n",
        "        noise_var: Variance of the noise distribution\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, prior_mean: jnp.DeviceArray, prior_cov: jnp.DeviceArray, noise_var: float):\n",
        "        self.prior_mean = prior_mean[:, jnp.newaxis] # column vector of shape (1, d)\n",
        "        self.prior_cov = prior_cov # matrix of shape (d, d)\n",
        "        # We initalize the prior distribution over the parameters using the given mean and covariance matrix\n",
        "        # In the formulas above this corresponds to m_0 (prior_mean) and S_0 (prior_cov)\n",
        "        self.prior = random.multivariate_normal(key,prior_mean, prior_cov)\n",
        "        \n",
        "        # We also know the variance of the noise\n",
        "        self.noise_var = noise_var # single float value\n",
        "        self.noise_precision = 1 / noise_var\n",
        "        \n",
        "        # Before performing any inference the parameter posterior equals the parameter prior\n",
        "        self.param_posterior = self.prior\n",
        "        # Accordingly, the posterior mean and covariance equal the prior mean and variance\n",
        "        self.post_mean = self.prior_mean # corresponds to m_N in formulas\n",
        "        self.post_cov = self.prior_cov # corresponds to S_N in formulas\n",
        "    def update_posterior(self, features: jnp.DeviceArray, targets: jnp.DeviceArray):\n",
        "        \"\"\"\n",
        "        Update the posterior distribution given new features and targets\n",
        "        \n",
        "        Args:\n",
        "            features: numpy array of features\n",
        "            targets: numpy array of targets\n",
        "        \"\"\"\n",
        "        # Reshape targets to allow correct matrix multiplication\n",
        "        # Input shape is (N,) but we need (N, 1)\n",
        "        targets = targets[:, jnp.newaxis]\n",
        "        \n",
        "        # Compute the design matrix, shape (N, 2)\n",
        "        design_matrix = self.compute_design_matrix(features)\n",
        "        # Update the covariance matrix, shape (2, 2)\n",
        "        design_matrix_dot_product = design_matrix.T.dot(design_matrix)\n",
        "        inv_prior_cov = jnp.linalg.inv(self.prior_cov)\n",
        "        self.post_cov = jnp.linalg.inv(inv_prior_cov +  self.noise_precision * design_matrix_dot_product)\n",
        "        \n",
        "        # Update the mean, shape (2, 1)\n",
        "        self.post_mean = self.post_cov.dot( \n",
        "                         inv_prior_cov.dot(self.prior_mean) + \n",
        "                         self.noise_precision * design_matrix.T.dot(targets))\n",
        "        \n",
        "        # Update the posterior distribution\n",
        "        self.param_posterior = random.multivariate_normal(key,mean= self.post_mean.flatten(), cov= self.post_cov)\n",
        "                \n",
        "    def compute_design_matrix(self, features: jnp.DeviceArray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute the design matrix. To keep things simple we use simple linear\n",
        "        regression and add the value phi_0 = 1 to our input data.\n",
        "        \n",
        "        Args:\n",
        "            features: numpy array of features\n",
        "        Returns:\n",
        "            design_matrix: numpy array of transformed features\n",
        "            \n",
        "        >>> compute_design_matrix(np.array([2, 3]))\n",
        "        np.array([[1., 2.], [1., 3.])\n",
        "        \"\"\"\n",
        "        n_samples = len(features)\n",
        "        phi_0 = jnp.ones(n_samples)\n",
        "        design_matrix = jnp.stack((phi_0, features), axis=1)\n",
        "        return design_matrix\n",
        "    \n",
        " \n",
        "    def predict(self, features: jnp.DeviceArray):\n",
        "        \"\"\"\n",
        "        Compute predictive posterior given new datapoint\n",
        "        \n",
        "        Args:\n",
        "            features: 1d numpy array of features\n",
        "        Returns:\n",
        "            pred_posterior: predictive posterior distribution\n",
        "        \"\"\"\n",
        "        design_matrix = self.compute_design_matrix(features)\n",
        "        \n",
        "        pred_mean = design_matrix.dot(self.post_mean)\n",
        "        pred_cov = design_matrix.dot(self.post_cov.dot(design_matrix.T)) + self.noise_var\n",
        "        \n",
        "        pred_posterior = pred_mean.flatten() + (random.normal(key)*pred_cov)\n",
        "        print(pred_posterior)\n",
        "        return pred_posterior\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def compute_function_labels(slope: float, intercept: float, noise_std_dev: float, data: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute target values given function parameters and data.\n",
        "    \n",
        "    Args:\n",
        "        slope: slope of the function (theta_1)\n",
        "        intercept: intercept of the function (theta_0)\n",
        "        data: input feature values (x)\n",
        "        noise_std_dev: standard deviation of noise distribution (sigma)\n",
        "        \n",
        "    Returns:\n",
        "        target values, either true or corrupted with noise\n",
        "    \"\"\"\n",
        "    \n",
        "    n_samples = len(data)\n",
        "    if noise_std_dev == 0: # Real function\n",
        "        return slope * data + intercept\n",
        "    else: # Noise corrupted\n",
        "        return slope * data + intercept + np.random.normal(0, noise_std_dev, n_samples)"
      ],
      "metadata": {
        "id": "S1yvW5E8SOq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_datapoints = 1000\n",
        "intercept = -0.7\n",
        "slope = 0.9\n",
        "noise_std_dev = 0.5\n",
        "noise_var = noise_std_dev**2\n",
        "lower_bound = -1.5\n",
        "upper_bound = 1.5\n",
        "# Generate dataset\n",
        "features = random.uniform(key,( n_datapoints,),minval= lower_bound, maxval= upper_bound)\n",
        "labels = compute_function_labels(slope, intercept, 0., features)\n",
        "noise_corrupted_labels = compute_function_labels(slope, intercept, noise_std_dev, features)"
      ],
      "metadata": {
        "id": "b9pzhWcaPOKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "plt.plot(features, labels, color='r', label=\"True values\")\n",
        "plt.scatter(features, noise_corrupted_labels, label=\"Noise corrupted values\")\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Labels\")\n",
        "plt.title(\"Real function along with noisy targets\")\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "KBcOD25zPXkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize BLR model\n",
        "noise_var = 0.2\n",
        "prior_mean = jnp.array([0, 0])\n",
        "prior_cov = 1/2 * jnp.identity(2)\n",
        "blr = BayesianLinearRegression(prior_mean, prior_cov, noise_var)"
      ],
      "metadata": {
        "id": "t_AroRSRWh4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_points_lst = [1, 5, 10, 50, 100, 200, 500, 1000]\n",
        "previous_n_points = 0\n",
        "for n_points in n_points_lst:\n",
        "    train_features = features[previous_n_points:n_points]\n",
        "    train_labels = noise_corrupted_labels[previous_n_points:n_points]\n",
        "    blr.update_posterior(train_features, train_labels)\n",
        "    previous_n_points = n_points"
      ],
      "metadata": {
        "id": "xyqn5SowWaTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blr.compute_design_matrix(features)\n",
        "blr.predict(features)"
      ],
      "metadata": {
        "id": "FmD3RMZuTrFE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}