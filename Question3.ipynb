{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question3.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNhaGOKg/7ZGbE1GxwGMBji",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parvbhargava/SRIP-Parv-Bhargava/blob/main/Question3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as np\n",
        "from jax import grad, jit, vmap, value_and_grad\n",
        "from jax import random\n",
        "# Generate key which is used to generate random numbers\n",
        "key = random.PRNGKey(1)"
      ],
      "metadata": {
        "id": "8RlexmDlNd7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " We will start by defining a simple PyTorch MNIST dataloader and afterwards set everything up to train. JAX is a purely functional programming framework. Hence, we cant wrap things in class instances or modules used for example from the PyTorch nn.Module semantics. We, therefore, will need the following functions to train a Multilayer Perceptron:\n",
        "\n",
        "\n",
        "\n",
        "*   A function that initializes the neural networks weights and returns a list of layer-specific parameters.\n",
        "*   A function that performs a forward pass through the network (e.g. by loop over the layers).\n",
        "*   A function that computes the cross-entropy loss of the predictions.\n",
        "*   A function that evaluates the accuracy of the network (simply for logging).\n",
        "\n",
        "*  A function that updates the parameters using some form gradient descent.\n",
        "\n",
        "All of these will then be tied together in a training loop. We start by importing some additional helpers (including the optimizers from JAX) and the dataset from PyTorch. Any other dataloader will do the job similarly as long as transform the inputs to JAX-NumPy arrays."
      ],
      "metadata": {
        "id": "_Os9vW-mOvbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ReLU(x):\n",
        "    \"\"\" Rectified Linear Unit (ReLU) activation function \"\"\"\n",
        "    return np.maximum(0, x)\n",
        "jit_ReLU = jit(ReLU)\n",
        "def relu_layer(params, x):\n",
        "    \"\"\" Simple ReLu layer for single sample \"\"\"\n",
        "    return ReLU(np.dot(params[0], x) + params[1])"
      ],
      "metadata": {
        "id": "klJcTvvISdYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import some additional JAX and dataloader helpers\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "from jax.scipy.special import logsumexp\n",
        "from jax.experimental import optimizers\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import time"
      ],
      "metadata": {
        "id": "lyOBVehPOmuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the PyTorch Data Loader for the training & test set\n",
        "batch_size = 100\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=batch_size, shuffle=True)  \n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "cqzEV8YBPZV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will need a function that initializes the weights in our MLP. We will pass a list of hidden layer sizes and the previously generated PRNG key. We need to split the key iteratively to generate the individual weights of our network. Letâ€™s see how this is done for a MLP that takes the flat MNIST image as an input (28 x 28 = 784) and has two hidden layers with 512 units (e.g. 784-512-512-10)."
      ],
      "metadata": {
        "id": "J-eJqLO2Qqyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_mlp(sizes, key):\n",
        "    \"\"\" Initialize the weights of all layers of a linear layer network \"\"\"\n",
        "    keys = random.split(key, len(sizes))\n",
        "    # Initialize a single layer with Gaussian weights -  helper function\n",
        "    def initialize_layer(m, n, key, scale=1e-2):\n",
        "        w_key, b_key = random.split(key)\n",
        "        return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
        "    return [initialize_layer(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
        "\n",
        "layer_sizes = [784, 512, 512, 10]\n",
        "# Return a list of tuples of layer weights\n",
        "params = initialize_mlp(layer_sizes, key)"
      ],
      "metadata": {
        "id": "saEOJ5tnQqYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define the forward pass through the network by iteratively looping over the layers and returning the log of the softmax output/predictions. Afterwards, we `vmap` the single case to create a batched version."
      ],
      "metadata": {
        "id": "zuPS2kdBQuGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(params, in_array):\n",
        "    \"\"\" Compute the forward pass for each example individually \"\"\"\n",
        "    activations = in_array\n",
        "\n",
        "    # Loop over the ReLU hidden layers\n",
        "    for w, b in params[:-1]:\n",
        "        activations = relu_layer([w, b], activations)\n",
        "\n",
        "    # Perform final trafo to logits\n",
        "    final_w, final_b = params[-1]\n",
        "    logits = np.dot(final_w, activations) + final_b\n",
        "    return logits - logsumexp(logits)\n",
        "\n",
        "# Make a batched version of the `predict` function\n",
        "batch_forward = vmap(forward_pass, in_axes=(None, 0), out_axes=0)"
      ],
      "metadata": {
        "id": "ZbQ-GIwPPmNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "famKvQiHQ3oD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(x, k, dtype=np.float32):\n",
        "    \"\"\"Create a one-hot encoding of x of size k \"\"\"\n",
        "    return np.array(x[:, None] == np.arange(k), dtype)\n",
        "\n",
        "def loss(params, in_arrays, targets):\n",
        "    \"\"\" Compute the multi-class cross-entropy loss \"\"\"\n",
        "    preds = batch_forward(params, in_arrays)\n",
        "    return -np.sum(preds * targets)\n",
        "\n",
        "def accuracy(params, data_loader):\n",
        "    \"\"\" Compute the accuracy for a provided dataloader \"\"\"\n",
        "    acc_total = 0\n",
        "    for batch_idx, (data, target) in enumerate(data_loader):\n",
        "        images = np.array(data).reshape(data.size(0), 28*28)\n",
        "        targets = one_hot(np.array(target), num_classes)\n",
        "\n",
        "        target_class = np.argmax(targets, axis=1)\n",
        "        predicted_class = np.argmax(batch_forward(params, images), axis=1)\n",
        "        acc_total += np.sum(predicted_class == target_class)\n",
        "    return acc_total/len(data_loader.dataset)"
      ],
      "metadata": {
        "id": "XT392mRHRjDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jit\n",
        "def update(params, x, y, opt_state):\n",
        "    \"\"\" Compute the gradient for a batch and update the parameters \"\"\"\n",
        "    value, grads = value_and_grad(loss)(params, x, y)\n",
        "    opt_state = opt_update(0, grads, opt_state)\n",
        "    return get_params(opt_state), opt_state, value\n",
        "\n",
        "# Defining an optimizer in Jax\n",
        "step_size = 1e-3\n",
        "opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
        "opt_state = opt_init(params)\n",
        "\n",
        "num_epochs = 10\n",
        "num_classes = 10"
      ],
      "metadata": {
        "id": "t_LZsSflR7S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_mnist_training_loop(num_epochs, opt_state, net_type=\"MLP\"):\n",
        "    \"\"\" Implements a learning loop over epochs. \"\"\"\n",
        "    # Initialize placeholder for loggin\n",
        "    log_acc_train, log_acc_test, train_loss = [], [], []\n",
        "\n",
        "    # Get the initial set of parameters\n",
        "    params = get_params(opt_state)\n",
        "\n",
        "    # Get initial accuracy after random init\n",
        "    train_acc = accuracy(params, train_loader)\n",
        "    test_acc = accuracy(params, test_loader)\n",
        "    log_acc_train.append(train_acc)\n",
        "    log_acc_test.append(test_acc)\n",
        "\n",
        "    # Loop over the training epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            if net_type == \"MLP\":\n",
        "                # Flatten the image into 784 vectors for the MLP\n",
        "                x = np.array(data).reshape(data.size(0), 28*28)\n",
        "            elif net_type == \"CNN\":\n",
        "                # No flattening of the input required for the CNN\n",
        "                x = np.array(data)\n",
        "            y = one_hot(np.array(target), num_classes)\n",
        "            params, opt_state, loss = update(params, x, y, opt_state)\n",
        "            train_loss.append(loss)\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        train_acc = accuracy(params, train_loader)\n",
        "        test_acc = accuracy(params, test_loader)\n",
        "        log_acc_train.append(train_acc)\n",
        "        log_acc_test.append(test_acc)\n",
        "        print(\"Epoch {} | T: {:0.2f} | Train A: {:0.3f} | Test A: {:0.3f}\".format(epoch+1, epoch_time,\n",
        "                                                                    train_acc, test_acc))\n",
        "\n",
        "    return train_loss, log_acc_train, log_acc_test\n",
        "\n",
        "\n",
        "train_loss, train_log, test_log = run_mnist_training_loop(num_epochs,\n",
        "                                                          opt_state,\n",
        "                                                          net_type=\"MLP\")"
      ],
      "metadata": {
        "id": "3oIreFaKSApc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}